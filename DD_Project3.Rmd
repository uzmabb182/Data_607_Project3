---
title: "Project 3"
author: 
 - Mubashira Qari
 - Deepika Dilip
 - Diana Plunkett
 - Moiya Josephs
output:
  html_document:
    theme: spacelab
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
The objective of this project was to compile data indicative of current data science skills in demand. We did this by creating multiple tables containing strings from blog posts along with populating data from actual job postings. 

In doing this, we sought to answer three questions:

1. How do the skills vary between the different job posting sites?
2. How does the skills looked for in job postings compare to the skills recommended in articles and blogs?
3. Is there any difference in the 3 categories: Languages, Fundamentals and Soft Skills.

# Step 1: Listing skills based on job postings

Our first main step was creating a table that populated data based on 1) skills in demand from data blogs and 2) job postings with the skills listed. 

```{r include = F}
library(rvest)
library(dplyr)
library(tidyverse)
library(knitr)
library(kableExtra)
library(plotly)
library(ggthemes)
library(ggplot2)
library(stringr)
library(treemap)
```


##  Performing Web Scraping to fetch the skills using the node selector

We used the `rvest` package to use a node selector and subset the skills of interest:

```{r}
# Start by reading a HTML page with read_html():
skills_page <- read_html("https://brainstation.io/career-guides/data-science-resume-examples#what-skills-should-you-put-on-a-data-science-resume")
skills_set1<- skills_page %>% html_nodes("ul:nth-child(4)") %>% html_elements("li")  %>% html_text2()
skills_set2<- skills_page %>% html_nodes("ul:nth-child(6)") %>% html_elements("li")  %>% html_text2()
```

From here, we can preview the vector:
```{r}
writeLines(skills_set1)
```

The next step we took was trimming extra spaces from the words in `skill_set1` and `skill_set2`.

```{r}
vector1 = c()
for (i in 6:19) {
  vector1[i-5] <- as.character(trimws(skills_set1[i], which = c("both")))
}

vector2 = c()
for (i in 3:34) {
  vector2[i-2] <- as.character(trimws(skills_set2[i], which = c("both")))
}
```

Here are the cleaned vectors:
```{r}
vector1 %>% writeLines()
```

```{r}
vector2 %>% writeLines()
```

*Soft Skills:*
We can start by reading a HTML page with `read_html()`:
```{r}
softskills_page <- read_html("https://zety.com/blog/data-scientist-resume-example")
```

From here, we can scrap these using node selector
```{r}

skills_set3<- softskills_page %>% html_nodes("td:nth-child(1) > p > a") %>% html_text2()

skills_set3[3:9]
```

As before, we now have a vector of soft skills:
```{r}
vector3 = c()

for (i in 3:9) {
  
  vector3[i-2] <- as.character(trimws(skills_set3[i], which = c("both")))
  
}

vector3
```

We can now combine the vectors with technical skills and the vectors containing soft skills.
```{r}
title_vector <- c(vector1, vector2, vector3)
```

## Adding the values in the columns
From there, we manually searched multiple job listing sites and populated tables based on the number of results:

```{r, echo = F}
indeed_jobs = c()
# adding jobs to an empty vector 

indeed_jobs = c('46,915','2,660','28,328','2,327',' 41,727','9,444','55,983','75,076','9,240','7,400',
                '49,922','3,706','72,630','5,368','12,265','85,808','1,578','2,277','102,663','14,354',
                '3,059','135,162','16,826','19,130','197,041','16,762','17,577','57,397','34,179','22,990',
                ' 37','9,539','2,472','63,047','868,715','117','7,211','18,860','525','169',
                '935','305','3,025','5,472','120,397','45','43,238','61,936','721,924','114,820',
                '316,760','1,135,873','1,278,165')
```

```{r, echo = F}
linkedin_jobs = c()
# adding jobs to an empty vector 

linkedin_jobs = c('580,822','3,329','135,443','216,875','119,686','510,656','53,452','186,554','21,617','112,298',
                  '98,477','3,452','5,587','60,006','175,906','898,552','315,245','771,041','197,040','812,683',
                  '867,841','800,863','69,694','29,898','1,196,586','27,913','163,737','263,855','64,198','70,445',
                  '19','16,685','4,266','156,649','360,852','627','67,961','60,296','638','180',
                  '1,146','375','14,710','9,081','234,937','38,932','24,555','4,571,013','663,670','45,321',
                  '543,385','1,915,666','1,056,725')
```

```{r, echo = F}
simplyhired_jobs = c()
# adding jobs to an empty vector 

simplyhired_jobs = c('27,586','1,233','13,702','44,456','37,409','87,187','33,666','34,298','21,562','11,574',
                     '26,019','2,237','6,353','14,068','61,044','99,354','193,995','40,280','47,118','34,411',
                     '69,038','63,254','9,906','10,482','104,201','6,411','11,880','31,166','14,219','9,865',
                     '34','4,243','1,038','32,330','497,140','33','23,610','7,756','345','96',
                     '630','219','1,391','3,158','66,652','5,255','22,061','41,747','372,740','60,142',
                     '157,402','548,264','547,690')
  
  
```

```{r, echo = F}
ziprecruiter_jobs = c()
# adding jobs to an empty vector 

ziprecruiter_jobs = c('1,685,359','434','623,648','288,970','170,268','2,047,037','261,054','317,549','316,611','116,528',
                      '111,577','2,220,185','3,460','22,966','20,558','457,959','54,686','291,574','243,188','78,526',
                      '292,644','202,555','45,434','39,475','269,004','27,009','29,676','130,988','42,417','36,291',
                      '14','14,633','487','89,157','488,192','13','70,727','17,436','1,508','154',
                      '110,348','168','213,055','13,278','188,515','17,266','39,603','5,547,167','839,660','145,056',
                      '355,482','34,357','1,159,558')

```

```{r}
tbl1.siteskills = data.frame(skill_title = title_vector,
                indeed = indeed_jobs,
                linkedin = linkedin_jobs,
                simplyhired = simplyhired_jobs,
                ziprecruiter = ziprecruiter_jobs
                )
head(tbl1.siteskills) %>% kable() %>% kableExtra::kable_styling(bootstrap_options = "basic")
```

We'll refer to this as `Table 1` moving forward. We can export this table into a working directory:
```{r}
# Get working directory path
path <- getwd()
path
write.csv(tbl1.siteskills, file.path(path, "skills_cleaned_data.csv"))
```

# Step 2: Integration with Existing Tables

From here, we can join this table with existing data we've scraped:

## Table 2: Skill Categories
```{r}
tbl2.skillcat = read.csv('https://raw.githubusercontent.com/uzmabb182/Data_607_Project3/main/SkillCategories.csv')
head(tbl2.skillcat) %>% kable() %>% kableExtra::kable_styling(bootstrap_options = "basic")
```

## Table 3: Recommended Skills
```{r}
tbl3.skillrec = read.csv('https://raw.githubusercontent.com/uzmabb182/Data_607_Project3/main/RecomendedSkills.csv')
head(tbl3.skillrec) %>% kable() %>% kableExtra::kable_styling(bootstrap_options = "basic")
```

## Table 4: Skill Recommendation Websites
```{r}
tbl4.skillsites = read.csv('https://raw.githubusercontent.com/uzmabb182/Data_607_Project3/main/RecommendationSites.csv')
head(tbl4.skillsites, 2) %>% kable() %>% kableExtra::kable_styling(bootstrap_options = "basic")
```

In total, we have the following tables:
```{r echo = F}

tbl1.siteskills = read.csv('https://raw.githubusercontent.com/uzmabb182/Data_607_Project3/main/JobPostingsSkillXref.csv')
tbl1.siteskills$`In.Recommended.Skills.` = NULL
lst.database = list(data.frame(Table1 =  names(tbl1.siteskills)), 
     data.frame(Table2 =  names(tbl2.skillcat)),
     data.frame(Table3 =  names(tbl3.skillrec)),
     data.frame(Table4 =  names(tbl4.skillsites)))
  
lst.database %>% kable() %>% kableExtra::kable_styling(bootstrap_options = "striped", full_width = F, position = "left")

```

# Step 3: Answering Questions

## Job Demand Distribution

We can start visualization by looking at the job demand distribution in `Table 1`. To do this, we'll need to melt the dataset prior to using `ggplot`:
```{r}
mlt.tbl1 = pivot_longer(tbl1.siteskills, cols = c("indeed", "linkedin", "simplyhired", "ziprecruiter"), values_to = "count")
mlt.tbl1 = mlt.tbl1 %>% mutate(count = as.integer(gsub("\\,", "", count)))
mlt.tbl1.merged = mlt.tbl1 %>% merge(tbl2.skillcat, by = c("skill_title", "SKILLID"), all.x = T)

mlt.tbl1.merged$name = str_to_sentence(mlt.tbl1.merged$name)

plt.sites = ggplot(mlt.tbl1.merged, aes(skill_title, count, fill = factor(skill_category))) + geom_bar(stat = "identity") + facet_wrap(~name, ncol = 1, scales = "free_y") + theme_minimal() + scale_fill_economist() + labs(x = "Skill", y = "Count", fill = "Skill Category") + theme(axis.text.x = element_text(size = 8, angle = 90, hjust = 1, vjust = 1), legend.position = "bottom") 

ggplotly(plt.sites) #Makes it interactive
```


This plot is a bit detailed, so we can visualize this distribution in more granular "buckets":

```{r}
mlt.tbl1.merged$skill_bucket = ifelse(mlt.tbl1.merged$skill_category == "Languages", "Languages", ifelse(mlt.tbl1.merged$skill_category == "Soft Skills", "Soft Skills", "Tech Data"))

plt.sites.aggregated = ggplot(mlt.tbl1.merged, aes(skill_title, count, fill = factor(skill_category))) + geom_bar(stat = "identity") + facet_wrap(~ skill_bucket + name, ncol = 2, scales = "free") + theme_minimal() + scale_fill_economist() + labs(x = "Skill", y = "Count", fill = "Skill Category") + theme(axis.text.x = element_text(size = 8, angle = 90, hjust = 1, vjust = 1), legend.position = "bottom") 

ggplotly(plt.sites.aggregated, width = 800, height = 1500)

```

If we don't want to stratify by job website and visualize an overview instead, it will appear as the following:

```{r message=F, warning = F}

tbl1.per_bucket = mlt.tbl1.merged %>% group_by(skill_title, skill_bucket) %>% summarise(count = sum(count))

plt.bucket = ggplot(tbl1.per_bucket, aes(skill_title, count, fill = factor(skill_bucket))) + geom_bar(stat = "identity") + facet_wrap(~ skill_bucket, ncol = 1, scales = "free") + theme_minimal() + scale_fill_economist() + labs(x = "Skill", y = "Count", fill = "Skill Category") + theme(axis.text.x = element_text(size = 8, angle = 90, hjust = 1, vjust = 1), legend.position = "bottom") 

ggplotly(plt.bucket, width = 800, height = 1500)
```

As an extra bit, we can run some basic statistical tests across the board to see if any particular skills are listed more often in one website than others. We'll start by running a proportions test and seeing which skillls differ the most across websites:

```{r echo = F}
tbl1.siteskills = tbl1.siteskills %>% mutate(indeed = as.integer(gsub("\\,", "", indeed)),
                           linkedin = as.integer(gsub("\\,", "", linkedin)),
                           simplyhired = as.integer(gsub("\\,", "", simplyhired)),
                           ziprecruiter = as.integer(gsub("\\,", "", ziprecruiter))) 
mat.site_skills = tbl1.siteskills %>% select(-c("SKILLID", "skill_title")) %>% data.matrix()
rownames(mat.site_skills) = tbl1.siteskills$skill_title


prop.test.pval = function(x)  { 
  y = prop.test(x, n = rep(sum(x), length(x))) 
  x = y$statistic
  return(x)
  }

vec.stat = apply(mat.site_skills, 1,  prop.test.pval)

prop.table = as.data.frame(prop.table(mat.site_skills, margin = 1)) %>% mutate(test.statistic = vec.stat) 

prop.table$test.statistic = round(prop.table$test.statistic, 2)

prop.table  %>%  arrange(desc(test.statistic)) %>% head(5)  %>% kable() %>% kable_styling(bootstrap_options = "basic") 
```

We'll do the same for those that differ the least among websites:
```{r}
prop.table  %>%  arrange(desc(test.statistic)) %>% tail(5)  %>% kable() %>% kable_styling(bootstrap_options = "basic") 
```

### Key observations:

* **Soft skills** were quite consistent throughout job listings. Communication in particular was emphasized across the board, followed by problem solving.
* With respect to **languages**, job descriptions appeared to emphasize querying languages (such as SQL) over programming languages such as Python and R. However, when excluding SQL, both Python and Javascript appeared in a considerable number of job listings.
* **Software packages** were different across the board. `Seaborn` and `plotly` had similar distributions across the job sites, but `Matplotlib` was seen more often in ZipRecruiter.


## Recommended Skills 

To answer analysis question 2, we need to categorize the skills in the recommendation websites. This can be achieved by joining table 3 and table 2 together on the common skill name. Some skill names are also capitalized which would be counted separately, so we changed the skill name columns in both websites to lower case to avoid this issue. 

```{r}
#Change the skills column to all lower case
tbl3.skillrec$Skill <- tolower(tbl3.skillrec$Skill)
tbl2.skillcat$skill_title <- tolower(tbl2.skillcat$skill_title)

# Join on common skill names
skillcat_rec <- tbl3.skillrec %>% inner_join(tbl2.skillcat, by = c( "Skill" = "skill_title"))
rec <- skillcat_rec %>% group_by(RecommendationID) %>% count(skill_category)
head(rec) %>% kable() %>% kableExtra::kable_styling(bootstrap_options = "basic")
```

Next, to graph the distribution of skill categories among the 12 chosen websites.
```{r}
rec.skillscat <-ggplot(rec, aes(skill_category ,n, fill = skill_category) ) + geom_col() + facet_grid(rows = vars(RecommendationID), scales= "free_y", switch = "y", space = "free_y") + coord_flip() + labs(x = "Recommendation Website", y = "Count") + 
theme(axis.text.x = element_text(size = 8, hjust = 1, vjust = 1), axis.text.y = element_blank(), axis.ticks.y=element_blank(), panel.grid.major = element_blank())

ggplotly(rec.skillscat)
```
The chart above shows the count for each skill category listed for each of the chosen websites we looked at. Website 1 gave the most recommended skills overall, while 8 had the least. Overall, this chart shows that Fundamentals, Languages, and Soft Skills are the most recommended skill categories across our websites.

To get a more detailed look, we can compare the websites that had the same skill category as their most recommended. 
### Fundamentals  

For recommendation websites 1, 2, 3, 5, and 9 the most popular skill category is fundamentals. It would be interesting to see what fundamental skills are listed specifically and if there is an overlap. 

```{r}
# Fundamentals 
fund.rec.skill <-skillcat_rec %>% select(Skill, RecommendationID, skill_category) %>% filter(RecommendationID == 1 | RecommendationID == 11 | RecommendationID == 2 | RecommendationID == 3 | RecommendationID == 5 | RecommendationID == 5, skill_category == "Fundamentals")
fund.rec.skill %>% 
  ggplot(aes(RecommendationID, Skill, color = Skill)) + geom_count() + facet_grid(cols = vars(RecommendationID)) + scale_x_discrete(breaks = c(1,9)) + theme(panel.grid.major = element_blank())
```

### Language

```{r}
#Language
lang.rec.skill <-skillcat_rec %>% select(Skill, RecommendationID, skill_category) %>% filter(RecommendationID == 1| RecommendationID == 9 | RecommendationID == 10 | RecommendationID == 12, skill_category == "Languages")
lang.rec.skill %>% 
  ggplot(aes(RecommendationID, Skill, color = Skill)) + geom_count() + facet_grid(cols = vars(RecommendationID)) + scale_x_discrete(breaks = c(1,9)) + theme(panel.grid.major = element_blank())
```
Websites 1, 9, 10, 12 top recommended skill category was languages. For the recommended languages, only three languages appeared in all three websites, SQL, R and python. 

### Soft Skills

```{r}
#Soft Skill
soft.rec.skill <-skillcat_rec %>% select(Skill, RecommendationID, skill_category) %>% filter(RecommendationID == 6| RecommendationID == 7)
soft.rec.skill %>% 
  ggplot(aes(RecommendationID, Skill, color = Skill)) + geom_count() + facet_grid(cols = vars(RecommendationID)) + scale_x_discrete(breaks = c(1,9)) + theme(panel.grid.major = element_blank())
```
For websites 6 and 7, the top recommended skill category was Soft Skills. The skills they had in common included, statistics, problem solving, machine learning, critical thinking and communication.

### Key Observations

The top recommended skill categories across the various websites offered a lot of overlap between the individual skills:  
* Websites 6 and 7 recommended soft skills had the most similarities in the skills they recommended, with a total of 6 skills in common. 
* For the websites that recommended language the most, they seemed to agree on three skills the most. 
* Fundamentals is interesting since two websites mentioned skills more than once (like programming and data wrangling) Between the five websites only machine learningand data visualization was mentioned in all.

Fundamentals was also the highest recommended overall followed by Languages and Soft Skills.

## Differences by Category  
First, we look at those skills that are in both the Job Postings and in the articles/blogs with recommended skills.
```{r}
color = c("#008080", "#428bca", "#d2afff", "#dab600", "#979aaa")
mlt.tbl1.merged$skill_title <- tolower(mlt.tbl1.merged$skill_title)
mlt.tbl1.merged$InRecSkills <- mlt.tbl1.merged$skill_title %in% tbl3.skillrec$Skill

mlt.tbl1.merged %>% 
  mutate(skill_category = 
           ifelse(skill_category =="Software" |
                    skill_category =="Database Management" |
                    skill_category == "Fundamentals" |
                    skill_category == "Mathematics", 
                  "Fundamentals", skill_category)) %>%
#filter out NAs
#now have job posting skills that were also in recommendations
  filter(InRecSkills) %>%
  treemap (index=c("skill_category","skill_title"),
           vSize = "count",
           type="index",
               fontsize.labels=c(15,12),                
    fontcolor.labels=c("white","blue"),
    fontface.labels=c(2,1),                 
    bg.labels=c("transparent"),              
    align.labels=list(
        c("center", "top"), 
        c("right", "bottom")),                                
    overlap.labels=0.5,                   
    inflate.labels=F,
  border.col=c("black","white"),  
    border.lwds=c(7,2),
  palette = color,
  title="Skills by Category in Both", 
    fontsize.title=10
  )


```

Next, we look at those skills that are in the Job Postings BUT NOT in the articles/blogs with recommended skills.
```{r}
color2 = c("#428bca","#d2afff","#dab600","#979aaa","#008080")

#which category has skills in job postings and in recomm
mlt.tbl1.merged %>% 
  mutate(skill_category = 
           ifelse(skill_category =="Software" |
                    skill_category =="Database Management" |
                    skill_category == "Fundamentals" |
                    skill_category == "Mathematics" |
                    skill_category =="Data Warehouse" |
                    skill_category =="Visualization",
                  "Fundamentals", skill_category)) %>%
  mutate(skill_category = 
           ifelse(skill_category =="Python Packages",
                  "Languages", skill_category)) %>%
#filter to keep only NAs
#now have job posting skills that were NOT in recommendations
  filter(!InRecSkills) %>%
  treemap (index=c("skill_category","skill_title"),
           vSize = "count",
           type="index",
               fontsize.labels=c(15,12),                
    fontcolor.labels=c("white","blue"),
    fontface.labels=c(2,1),                 
    bg.labels=c("transparent"),              
    align.labels=list(
        c("center", "top"), 
        c("right", "bottom")),                                
    overlap.labels=0.5,                   
    inflate.labels=F,
  border.col=c("black","white"),  
    border.lwds=c(7,2),
  palette = color2,
  title="Skills by Category in Posting only", 
    fontsize.title=10,
  )


```

# Conclusion
## What the Data Sci Skills Articles Got Right (and didn't) 
There was a good amount of overlap in the skills for Data Science recommended by articles and the also in job postings:    

The largest group in this case is the Soft Skills, specifically, Communication, Problem Solving, Collaboration and Critical Thinking.  Communication and Problem Solving were also the soft skills mentioned most consistently in job posting accross all the job sites.  Those two (Communication and Problem Solving), along with Critical Thinking, were also common in those Skill Recommendation articles that mentioned soft skills.      

After that came Fundamentals, with skills like Programming, Data Analysis and Excel (the top 3 in this category).  These were much more varied in the job postings, and may be specific to a particular industry or driven by what hiring companies are already using.  

The smallest group of skills mentioned in both job postings and recommendation sites was Languages, with SQL, Python, Java and C++ as the top 4.  SQL was the top Language seen in the job postings, with Python and Java(script) also seen frequently.  The recommendations added R to this list.   

When we look at the skills the articles MISSED recommending, we see that Fundamentals is just slightly bigger than Soft Skills, and Languages is just a bit smaller.  Top skills by categories that were MISSED by the articles are:    
Fundamentals: Hypothesis Testing    
Soft Skills: Interpersonal Skills (which may be just a broader catch all for some of the other skills already mentioned)    
Languages: JavaScript, HTML, C# and C

Given this, if you are going to write about skills needed in this field - we suggest taking a closer look at skills in current job postings to make sure you cover the bases effectively. 

If you are a Data Scientist who is job hunting, you would likely benefit from emphisising a balance between hard and soft skills.  On the soft skill side, Communication and Problem Solving seem pretty universally desired. In Languages, SQL is clearly a desired skill, with Python, Java/JavaScript HTML and C#/C as the next tier.  There was more variety on the Fundimental skills, and here we recommend looking more specifically at the industry or organization you are targeting in your search.     

