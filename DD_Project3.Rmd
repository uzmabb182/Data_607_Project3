---
title: "Project 3"
output:
  html_document:
    theme: spacelab
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
The objective of this project was to compile data indicative of current data science skills in demand. We did this by creating multiple tables containing strings from blog posts along with populating data from actual job postings. 

In doing this, we sought to answer three questions:

1. How do the skills vary between the different job posting sites?
2. How does the skills looked for in job postings compare to the skills recommended in articles and blogs?
3. Is there any difference in the 3 categories: Languages, Tech Data and Soft Skills.

# Step 1: Listing skills based on job postings

Our first main step was creating a table that populated data based on 1) skills in demand from data blogs and 2) job postings with the skills listed. 

```{r include = F}
library(rvest)
library(dplyr)
library(tidyverse)
library(knitr)
library(kableExtra)
library(plotly)
library(ggthemes)
library(ggplot2)
library(stringr)
```


##  Performing Web Scraping to fetch the skills using the node selector

We used the `rvest` package to use a node selector and subset the skills of interest:

```{r}
# Start by reading a HTML page with read_html():
skills_page <- read_html("https://brainstation.io/career-guides/data-science-resume-examples#what-skills-should-you-put-on-a-data-science-resume")
skills_set1<- skills_page %>% html_nodes("ul:nth-child(4)") %>% html_elements("li")  %>% html_text2()
skills_set2<- skills_page %>% html_nodes("ul:nth-child(6)") %>% html_elements("li")  %>% html_text2()
```

From here, we can preview the vector:
```{r}
writeLines(skills_set1)
```

The next step we took was trimming extra spaces from the words in `skill_set1` and `skill_set2`.

```{r}
vector1 = c()
for (i in 6:19) {
  vector1[i-5] <- as.character(trimws(skills_set1[i], which = c("both")))
}

vector2 = c()
for (i in 3:34) {
  vector2[i-2] <- as.character(trimws(skills_set2[i], which = c("both")))
}
```

Here are the cleaned vectors:
```{r}
vector1 %>% writeLines()
```

```{r}
vector2 %>% writeLines()
```

*Soft Skills:*
We can start by reading a HTML page with `read_html()`:
```{r}
softskills_page <- read_html("https://zety.com/blog/data-scientist-resume-example")
```

From here, we can scrap these using node selector
```{r}

skills_set3<- softskills_page %>% html_nodes("td:nth-child(1) > p > a") %>% html_text2()

skills_set3[3:9]
```

As before, we now have a vector of soft skills:
```{r}
vector3 = c()

for (i in 3:9) {
  
  vector3[i-2] <- as.character(trimws(skills_set3[i], which = c("both")))
  
}

vector3
```

We can now combine the vectors with technical skills and the vectors containing soft skills.
```{r}
title_vector <- c(vector1, vector2, vector3)
```

## Adding the values in the columns
From there, we manually searched multiple job listing sites and populated tables based on the number of results:

```{r, echo = F}
indeed_jobs = c()
# adding jobs to an empty vector 

indeed_jobs = c('46,915','2,660','28,328','2,327',' 41,727','9,444','55,983','75,076','9,240','7,400',
                '49,922','3,706','72,630','5,368','12,265','85,808','1,578','2,277','102,663','14,354',
                '3,059','135,162','16,826','19,130','197,041','16,762','17,577','57,397','34,179','22,990',
                ' 37','9,539','2,472','63,047','868,715','117','7,211','18,860','525','169',
                '935','305','3,025','5,472','120,397','45','43,238','61,936','721,924','114,820',
                '316,760','1,135,873','1,278,165')
```

```{r, echo = F}
linkedin_jobs = c()
# adding jobs to an empty vector 

linkedin_jobs = c('580,822','3,329','135,443','216,875','119,686','510,656','53,452','186,554','21,617','112,298',
                  '98,477','3,452','5,587','60,006','175,906','898,552','315,245','771,041','197,040','812,683',
                  '867,841','800,863','69,694','29,898','1,196,586','27,913','163,737','263,855','64,198','70,445',
                  '19','16,685','4,266','156,649','360,852','627','67,961','60,296','638','180',
                  '1,146','375','14,710','9,081','234,937','38,932','24,555','4,571,013','663,670','45,321',
                  '543,385','1,915,666','1,056,725')
```

```{r, echo = F}
simplyhired_jobs = c()
# adding jobs to an empty vector 

simplyhired_jobs = c('27,586','1,233','13,702','44,456','37,409','87,187','33,666','34,298','21,562','11,574',
                     '26,019','2,237','6,353','14,068','61,044','99,354','193,995','40,280','47,118','34,411',
                     '69,038','63,254','9,906','10,482','104,201','6,411','11,880','31,166','14,219','9,865',
                     '34','4,243','1,038','32,330','497,140','33','23,610','7,756','345','96',
                     '630','219','1,391','3,158','66,652','5,255','22,061','41,747','372,740','60,142',
                     '157,402','548,264','547,690')
  
  
```

```{r, echo = F}
ziprecruiter_jobs = c()
# adding jobs to an empty vector 

ziprecruiter_jobs = c('1,685,359','434','623,648','288,970','170,268','2,047,037','261,054','317,549','316,611','116,528',
                      '111,577','2,220,185','3,460','22,966','20,558','457,959','54,686','291,574','243,188','78,526',
                      '292,644','202,555','45,434','39,475','269,004','27,009','29,676','130,988','42,417','36,291',
                      '14','14,633','487','89,157','488,192','13','70,727','17,436','1,508','154',
                      '110,348','168','213,055','13,278','188,515','17,266','39,603','5,547,167','839,660','145,056',
                      '355,482','34,357','1,159,558')

```

```{r}

tbl1.siteskills = data.frame(skill_title = title_vector,
                indeed = indeed_jobs,
                linkedin = linkedin_jobs,
                simplyhired = simplyhired_jobs,
                ziprecruiter = ziprecruiter_jobs
                )
head(tbl1.siteskills) %>% kable() %>% kableExtra::kable_styling(bootstrap_options = "basic")
```

We'll refer to this as `Table 1` moving forward. We can export this table into a working directory:
```{r}
# Get working directory path
path <- getwd()
path
write.csv(tbl1.siteskills, file.path(path, "skills_cleaned_data.csv"))
```

# Step 2: Integration with Existing Tables

From here, we can join this table with existing data we've scraped:

## Table 2: Skill Categories
```{r}
tbl2.skillcat = read.csv('SkillCategories.csv')
head(tbl2.skillcat) %>% kable() %>% kableExtra::kable_styling(bootstrap_options = "basic")
```

## Table 3: Recommended Skills
```{r}
tbl3.skillrec = read.csv('RecomendedSkills.csv')
head(tbl3.skillrec) %>% kable() %>% kableExtra::kable_styling(bootstrap_options = "basic")
```

## Table 4: Skill Recommendation Websites
```{r}
tbl4.skillsites = read.csv('RecommendationSites.csv')
head(tbl4.skillsites, 2) %>% kable() %>% kableExtra::kable_styling(bootstrap_options = "basic")
```

In total, we have the following tables:
```{r echo = F}

tbl1.siteskills = read.csv('JobPostingsSkillXref.csv')
tbl1.siteskills$`In.Recommended.Skills.` = NULL
lst.database = list(data.frame(Table1 =  names(tbl1.siteskills)), 
     data.frame(Table2 =  names(tbl2.skillcat)),
     data.frame(Table3 =  names(tbl3.skillrec)),
     data.frame(Table4 =  names(tbl4.skillsites)))
  
lst.database %>% kable() %>% kableExtra::kable_styling(bootstrap_options = "striped", full_width = F, position = "left")

```

# Step 3: Answering Questions

## Job Demand Distribution

We can start visualization by looking at the job demand distribution in `Table 1`. To do this, we'll need to melt the dataset prior to using `ggplot`:
```{r}
mlt.tbl1 = pivot_longer(tbl1.siteskills, cols = c("indeed", "linkedin", "simplyhired", "ziprecruiter"), values_to = "count")
mlt.tbl1 = mlt.tbl1 %>% mutate(count = as.integer(gsub("\\,", "", count)))
mlt.tbl1.merged = mlt.tbl1 %>% merge(tbl2.skillcat, by = c("skill_title", "SKILLID"), all.x = T)

mlt.tbl1.merged$name = str_to_sentence(mlt.tbl1.merged$name)

plt.sites = ggplot(mlt.tbl1.merged, aes(skill_title, count, fill = factor(skill_category))) + geom_bar(stat = "identity") + facet_wrap(~name, ncol = 1, scales = "free_y") + theme_minimal() + scale_fill_economist() + labs(x = "Skill", y = "Count", fill = "Skill Category") + theme(axis.text.x = element_text(size = 8, angle = 90, hjust = 1, vjust = 1), legend.position = "bottom") 

ggplotly(plt.sites) #Makes it interactive
```


This plot is a bit detailed, so we can visualize this distribution in more granular "buckets":

```{r}
mlt.tbl1.merged$skill_bucket = ifelse(mlt.tbl1.merged$skill_category == "Languages", "Languages", ifelse(mlt.tbl1.merged$skill_category == "Soft Skills", "Soft Skills", "Tech Data"))

plt.sites.aggregated = ggplot(mlt.tbl1.merged, aes(skill_title, count, fill = factor(skill_category))) + geom_bar(stat = "identity") + facet_wrap(~ skill_bucket + name, ncol = 2, scales = "free") + theme_minimal() + scale_fill_economist() + labs(x = "Skill", y = "Count", fill = "Skill Category") + theme(axis.text.x = element_text(size = 8, angle = 90, hjust = 1, vjust = 1), legend.position = "bottom") 

ggplotly(plt.sites.aggregated, width = 800, height = 1500)

```

If we don't want to stratify by job website and visualize an overview instead, it will appear as the following:

```{r message=F, warning = F}

tbl1.per_bucket = mlt.tbl1.merged %>% group_by(skill_title, skill_bucket) %>% summarise(count = sum(count))

plt.bucket = ggplot(tbl1.per_bucket, aes(skill_title, count, fill = factor(skill_bucket))) + geom_bar(stat = "identity") + facet_wrap(~ skill_bucket, ncol = 1, scales = "free") + theme_minimal() + scale_fill_economist() + labs(x = "Skill", y = "Count", fill = "Skill Category") + theme(axis.text.x = element_text(size = 8, angle = 90, hjust = 1, vjust = 1), legend.position = "bottom") 

ggplotly(plt.bucket, width = 800, height = 1500)
```

As an extra bit, we can run some basic statistical tests across the board to see if any particular skills are listed more often in one website than others. We'll start by running a proportions test and seeing which skillls differ the most across websites:

```{r echo = F}
tbl1.siteskills = tbl1.siteskills %>% mutate(indeed = as.integer(gsub("\\,", "", indeed)),
                           linkedin = as.integer(gsub("\\,", "", linkedin)),
                           simplyhired = as.integer(gsub("\\,", "", simplyhired)),
                           ziprecruiter = as.integer(gsub("\\,", "", ziprecruiter))) 
mat.site_skills = tbl1.siteskills %>% select(-c("SKILLID", "skill_title")) %>% data.matrix()
rownames(mat.site_skills) = tbl1.siteskills$skill_title


prop.test.pval = function(x)  { 
  y = prop.test(x, n = rep(sum(x), length(x))) 
  x = y$statistic
  return(x)
  }

vec.stat = apply(mat.site_skills, 1,  prop.test.pval)

prop.table = as.data.frame(prop.table(mat.site_skills, margin = 1)) %>% mutate(test.statistic = vec.stat) 

prop.table$test.statistic = round(prop.table$test.statistic, 2)

prop.table  %>%  arrange(desc(test.statistic)) %>% head(5)  %>% kable() %>% kable_styling(bootstrap_options = "basic") 
```

We'll do the same for those that differ the least among websites:
```{r}
prop.table  %>%  arrange(desc(test.statistic)) %>% tail(5)  %>% kable() %>% kable_styling(bootstrap_options = "basic") 
```

### Key observations:

* **Soft skills** were quite consistent throughout job listings. Communication in particular was emphasized across the board, followed by problem solving.
* With respect to **languages**, job descrptions appeared to emphasize querying languages (such as SQL) over programming languages such as Python and R. However, when excluding SQL, both Python and Javascript appeared in a considerable number of job listings.
* **Software packages** were different across the board. `Seaborn` and `plotly` had similar distributions across the job sites, but `Matplotlib` was seen more often in ZipRecruiter.
<<<<<<< Updated upstream
=======

>>>>>>> Stashed changes


## Recommended Skills 

Next to categorize the skills in the recommended websites we need to join table 3 and table 2 together, joining on the common skill name. Some skill names are also capitalized which would be counted separately, so we changed the skill name columns in both websites to lower case to avoid this issue. 

```{r}
#Change the skills column to all lower case
tbl3.skillrec$Skill <- tolower(tbl3.skillrec$Skill)
tbl2.skillcat$skill_title <- tolower(tbl2.skillcat$skill_title)

# Join on common skill names, full join or inner?
skillcat_rec <- tbl3.skillrec %>% inner_join(tbl2.skillcat, by = c( "Skill" = "skill_title"))
rec <- skillcat_rec %>% group_by(RecommendationID) %>% count(skill_category)
head(rec) %>% kable() %>% kableExtra::kable_styling(bootstrap_options = "basic")
```

Next to graph the distribution of skill categories among the 12 chosen websites.
```{r}
# WIP Plan to make this interactive as well colors are hard to distinguish
ggplot(rec, aes(skill_category ,n, fill = skill_category) ) + geom_col() + facet_grid(rows = vars(RecommendationID), scales= "free_y", switch = "y", space = "free_y") + coord_flip() + labs(x = "RecommendationID", y = "Count") + 
theme(axis.text.x = element_text(size = 8, hjust = 1, vjust = 1), axis.text.y = element_blank(), axis.ticks.y=element_blank())

```
(WIP: Use this to compare recommended skill categories vs job postings, question #2. Fundamentals seem to be a more popular category than in the job site postings. Need annotatoins)

(Maybe we can get a more detailed look of what each skill is in the category that they are looking for for the most popular categories to give us more analysis. I started a little of it to see if its worth investigating.)

```{r}
skillcat_rec %>% filter(RecommendationID == 1 | RecommendationID == 9, skill_category == "Fundamentals")

# Data wrangling appears twice for RecommendationID 1, is this an error?
```

```{r}
# WIP Need to add color, maybe a different graphical output type. Trying to see common skills between articles

# Fundamentals 
skillcat_rec %>%
  filter(RecommendationID == 9 | RecommendationID == 1 & skill_category == "Fundamentals") %>% 
  ggplot(aes(RecommendationID, Skill)) + geom_count() + facet_grid(cols = vars(RecommendationID)) + scale_x_discrete(breaks = c(1,9))
```

```{r}
# WIP categorise skills into the three general terms to potentially answer #3
<<<<<<< Updated upstream
```
=======
```

## Differences by Category
First, we look at those skills that are in both the Job Postings and in the articles/blogs with recommended skills
```{r}
color = c("#008080", "#428bca", "#d2afff", "#dab600", "#979aaa")

mlt.tbl1.merged %>% 
#change the blanks to NA 
  mutate(In.Recommended.Skills.=
           ifelse(In.Recommended.Skills.=="", 
                  NA, In.Recommended.Skills.)) %>%
  mutate(skill_category = 
           ifelse(skill_category =="Software" |
                    skill_category =="Database Management" |
                    skill_category == "Fundamentals" |
                    skill_category == "Mathematics", 
                  "Fundamentals", skill_category)) %>%
#filter out NAs
#now have job posting skills that were also in recommendations
  filter(!is.na(In.Recommended.Skills.)) %>%
  treemap (index=c("skill_category","skill_title"),
           vSize = "count",
           type="index",
               fontsize.labels=c(15,12),                
    fontcolor.labels=c("white","blue"),
    fontface.labels=c(2,1),                 
    bg.labels=c("transparent"),              
    align.labels=list(
        c("center", "top"), 
        c("right", "bottom")),                                
    overlap.labels=0.5,                   
    inflate.labels=F,
  border.col=c("black","white"),  
    border.lwds=c(7,2),
  palette = color,
  title="Skills by Category in Both", 
    fontsize.title=10
  )


```
<I cannot get the Title to show on the above>

Next, we look at those skills that are in the Job Postings BUT NOT in the articles/blogs with recommended skills
```{r}
color2 = c("#428bca","#d2afff","#dab600","#979aaa","#008080")

#which category has skills in job postings and in recomm
mlt.tbl1.merged %>% 
#change the blanks to NA 
  mutate(In.Recommended.Skills.=
           ifelse(In.Recommended.Skills.=="", 
                  NA, In.Recommended.Skills.)) %>%
  mutate(skill_category = 
           ifelse(skill_category =="Software" |
                    skill_category =="Database Management" |
                    skill_category == "Fundamentals" |
                    skill_category == "Mathematics" |
                    skill_category =="Data Warehouse" |
                    skill_category =="Visualization",
                  "Fundamentals", skill_category)) %>%
  mutate(skill_category = 
           ifelse(skill_category =="Python Packages",
                  "Languages", skill_category)) %>%
#filter to keep only NAs
#now have job posting skills that were NOT in recommendations
  filter(is.na(In.Recommended.Skills.)) %>%
  treemap (index=c("skill_category","skill_title"),
           vSize = "count",
           type="index",
               fontsize.labels=c(15,12),                
    fontcolor.labels=c("white","blue"),
    fontface.labels=c(2,1),                 
    bg.labels=c("transparent"),              
    align.labels=list(
        c("center", "top"), 
        c("right", "bottom")),                                
    overlap.labels=0.5,                   
    inflate.labels=F,
  border.col=c("black","white"),  
    border.lwds=c(7,2),
  palette = color2,
  title="Skills by Category in Posting only", 
    fontsize.title=10,
  )


```

## What the Data Sci Skill Articles Got Right (and didn't) 
There was a good amount of overlap in the skills for Data Science recommended by articles and the job postings:    

The largest group in this case is the Soft Skills, specifically, Communication, Problem Solving, Collaboration and Critical Thinking.  After that came Fundamentals, with skills like Programming, Data Analysis and Excel (the top 3 in this category).  The smallest group here was Languages, with SQL, Python, Java and C++ as the top 4. 

When we look at the skills the articles MISSED recommending, we see that Fundamentals is just slightly bigger than Soft Skills, and Languages is just a bit smaller.  Top skills by categories that were MISSED by the articles are:    
Fundamentals: Hypothesis Testing    
Soft Skills: Interpersonal Skills    
Languages: JavaScript, HTML, C# and C
>>>>>>> Stashed changes
